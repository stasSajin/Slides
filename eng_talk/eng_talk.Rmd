---
title: "Machine Learning Metrics"
subtitle: "for lenders"
author: "Stas Sajin"
date: "04/01/2017"
output:
  xaringan::moon_reader:
    self_contained: TRUE
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

class: left, top

# Goals:

### 1. How do you know that a model works?
### 2. Ask the right/weird/uncomfortable questions.

---
## Ask the right/weird/uncomfortable questions

#### Example of model bias and flexibility:

.top[![50](https://static01.nyt.com/images/2016/09/08/upshot/arizona-election-forecast-1473366534539/arizona-election-forecast-1473366534539-videoSixteenByNineJumbo1600.png)]

- use data that is biased 

  * throwing away data that has missing values

  * putting more weight on data that you "feel" should have more weight

- being a tryhard
  
  * trying hard to look for a particular outcome without checks in place, to the point where it leads to overfitting.
---

class: left, top

### Outline:

##### Classification Metrics

- Accuracy
- Sensitivity (i.e., recall)
- Specificity 
- Positive Predictive Value (i.e., precision)
- F1
- Matthew's correlation coefficient (MCC)
- Cohen's Kappa
- Logarithmic Loss
- Receiver operating characteristic (ROC) curve
- Area under the (ROC) curve (AUC)


##### Regression Metrics
- Mean Squared Error (MSE)
- Root mean squared error (RMSE)
- Mean absolute error (MAE)
- Root mean squared log error (RMSLE)

This is not a comprehensive list and the easiest way to expand the list is to modify any of the above metrics with weighting or cost schemes. 

---
class: left

# The Setting

You finally decide to invest your money through one of the P2P lending platforms. One thing you are worried about is defaults, so you decide to built a model for predicting who is going to default based on information provided in the loan listing.


---
# Data

General Fake Data Information:
* 10000 observations
* 5 noise predictors and 13 real predictors (a combination of linear and nonlinear predictors)

```{r, echo=FALSE, fig.width= 7, fig.height=4, dev='svg', warning=FALSE, fig.align='center'}
# generate the random data
pacman::p_load(dplyr, ggplot2, caret, tidyr, purrr, viridis)
set.seed(123)
training <- twoClassSim(n = 10000, noiseVars = 5, corrValue = .8, linearVars = 5) 
levels(training$Class) <- c("default", "non-default")

# plot a sample dataset that shows the relationship between a noise variable, a twofactor variable a linear variable, and a non-linear variable.
plot_df <- training %>% select(TwoFactor1, TwoFactor2, Linear3, Linear2, Nonlinear3, Noise3, Class)
colnames(plot_df) <- c("Real or Not1?", "Real or Not2?", "Real or Not3?", "Real or Not4?",
                       "Real or Not5?", "Real or Not6?", "Outcome")
plot_df <- gather(plot_df, key = predictor, value = value, -Outcome)

# examine the distributions for each variable
ggplot(plot_df, aes(x = value, color = Outcome)) + 
  geom_density() + 
  facet_wrap(~predictor) +
  theme_bw() +
  labs(title = "Which of the following looks like a real predictor to you?")
```

---
# Scenarios

### Scenario 1

We don't have class imbalance in our data. About 50% of observations are defaults.

### Scenario 2

We have a small class imbalance issue, where about 20% of the loans represent defaults

### Scenario 3

We have a severe class imbalance issue, where about 1% of the loans represent defaults.
---

## The Model

- The model we use is irrelevant. For our purposes, we will be using decision trees. 

- We also ignore common checks for model evaluation. 

```{r, echo = FALSE}
# create data for the 3 scenarios
scenario1 <- twoClassSim(n = 10000, noiseVars = 5,
                                 corrValue = .8, linearVars = 5, intercept = -4) 
scenario1$scenario <- "scenario1"
scenario2 <- twoClassSim(n = 10000, noiseVars = 5,
                                     corrValue = .8, linearVars = 5, intercept = 1.7) 
scenario2$scenario <- "scenario2"
scenario3 <- twoClassSim(n = 10000, noiseVars = 5,
                                    corrValue = .8, linearVars = 5,
                                    intercept = 13) 
scenario3$scenario <- "scenario3"

#table(scenario1$Class)/nrow(scenario1)
#table(scenario2$Class)/nrow(scenario2)
#table(scenario3$Class)/nrow(scenario3)
data <- rbind(scenario1, scenario2, scenario3)

by_scenario <- data %>%
  group_by(scenario) %>%
  nest()

# function to be used in modeling
rpart_model <- function(df){
  rpart::rpart(Class ~ ., data = df)
}

# apply the model to every scenario
by_scenario <- by_scenario %>%
  mutate(model = map(data, rpart_model))

# add predictions 
by_scenario <- by_scenario %>%
  ungroup() %>%
  mutate(predictions = map(model, predict),
         predictions = map(predictions, as.data.frame))

data_with_pred <- by_scenario %>% unnest(predictions)
data_with_pred$Class <- data$Class

# generate A, B, C, D columns
data_with_pred <- data_with_pred %>%
  mutate(Outcome =
           case_when(
             .$Class1 >= .5 & .$Class == "Class1" ~ "true_positive",
             .$Class1 >= .5 & .$Class == "Class2" ~ "false_positive",
             .$Class1 < .5 & .$Class == "Class1" ~ "false_negative",
             .$Class1 < .5 & .$Class == "Class2" ~ "true_negative"))
```

---

# Accuracy

Definition: Measures the proportion of occurences that you got right

              Reference	
    Predicted	default	non-default
       default	 TP	    FP
    non-default	FN	    TN

- TP = True Positives
- FP = False Positives (false alarm)
- FN = False Negatives (misses)
- TN = True Negatives

##### Formula:

$$Accuracy = \frac{(TP+TN)}{(TP+FP+FN+TN)}$$
---

# Accuracy

```{r, echo=FALSE, fig.width= 7, fig.height=4, dev='svg', warning=FALSE, fig.align='center'}
# calculate accuracy for each scenario and plot it
plot_df <- data_with_pred %>%
  group_by(scenario) %>%
  summarise(fp = sum(Outcome == "false_positive"),
            fn = sum(Outcome == "false_negative"),
            tn = sum(Outcome == "true_negative"),
            tp = sum(Outcome == "true_positive")) %>%
  mutate(accuracy = (tp+tn)/(tp+tn+fp+fn))

ggplot(plot_df, aes(x = scenario, y =accuracy, fill = scenario)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label = accuracy), stat = "identity", vjust = -0.5, size = 12) +
    theme_bw() +
    scale_fill_viridis(discrete = TRUE) +
    scale_y_continuous(limits = c(0, 1.2), breaks = seq(0, 1, 0.1), labels = seq(0, 1, 0.1)) +
    labs(title = toupper("Accuracy for each scenario"),
         y = toupper("Accuracy"),
         x = toupper("Scenario")) +
    theme(legend.position="none") +
    theme(text = element_text(size=20))
```

---

## Sensitivity (aka Recall, TPR, hit rate)

Definition: How many defaulters have you identified out of all the people that have acctually defaulted?

Formula:

$$Sensitivity = \frac{TP}{TP+FN} = \frac{\text{number of true positives}}{\text{total number of actual defaults }}$$

```{r, echo=FALSE, fig.width= 7, fig.height=3.5, dev='svg', warning=FALSE, fig.align='top'}
plot_df <- plot_df %>%
  mutate(sens = round(tp/(tp+fn),2))

ggplot(plot_df, aes(x = scenario, y =sens, fill = scenario)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label = sens), stat = "identity", vjust = -0.5, size = 12) +
    theme_bw() +
    scale_fill_viridis(discrete = TRUE) +
    scale_y_continuous(limits = c(0, 1.2), breaks = seq(0, 1, 0.1), labels = seq(0, 1, 0.1)) +
    labs(title = toupper("Sensitivity for each scenario"),
         y = toupper("Sensitivity"),
         x = toupper("Scenario")) +
    theme(legend.position="none") +
    theme(text = element_text(size=20))
```

---

## Sensitivity (aka Recall, TPR, hit rate)

Do you think a lender should optimize for this metric? How can you achieve perfect scores on this metric?

???
A model can maximize this metric by scoring everybody as a default case.

---

## Likewise, Specificity (TNR)

Definition: Out of all the non-defaults, how many have you identified correctly as non-defaults? Esentially, you look at rate rate of false alarms.

Formula: 

$$Specificity = \frac{TN}{TN+FP} = \frac{\text{number of true negatives}}{\text{total number of actual non-defaults}}$$
```{r, echo=FALSE, fig.width= 7, fig.height=3.5, dev='svg', warning=FALSE, fig.align='center'}
plot_df <- plot_df %>%
  mutate(speci = round(tn/(tn+fp),2))

ggplot(plot_df, aes(x = scenario, y =speci, fill = scenario)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label = speci), stat = "identity", vjust = -0.5, size = 12) +
    theme_bw() +
    scale_fill_viridis(discrete = TRUE) +
    scale_y_continuous(limits = c(0, 1.2), breaks = seq(0, 1, 0.1), labels = seq(0, 1, 0.1)) +
    labs(title = toupper("Specificity for each scenario"),
         y = toupper("Specificity"),
         x = toupper("Scenario")) +
    theme(legend.position="none") +
    theme(text = element_text(size=20))
```

Is this good or bad?

???
A model can maximize this metric by scoring everyone as a non-default case. This test answer how good is one at avoiding false alarms.

---

### Precision (i.e., the land of grasped opportunities; aka, Positive Predictive Value)

Definition: Out of all the observations that you scored as defaults, how many have acctually defaulted?

Formula: 

$$Precision = \frac{TP}{TP+FP} = \frac{\text{number of positive defaults}}{\text{total number of predicted defaults}}$$

```{r, echo=FALSE, fig.width= 7, fig.height=3.5, dev='svg', warning=FALSE, fig.align='center'}
plot_df <- plot_df %>%
  mutate(precision = round(tp/(tp+fp),2))

ggplot(plot_df, aes(x = scenario, y =precision, fill = scenario)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label = precision), stat = "identity", vjust = -0.5, size = 12) +
    theme_bw() +
    scale_fill_viridis(discrete = TRUE) +
    scale_y_continuous(limits = c(0, 1.2), breaks = seq(0, 1, 0.1), labels = seq(0, 1, 0.1)) +
    labs(title = toupper("Specificity for each scenario"),
         y = toupper("Specificity"),
         x = toupper("Scenario")) +
    theme(legend.position="none") +
    theme(text = element_text(size=20))
```

Is this good or bad?

???
A model can maximize this metric by scoring everyone as a non-default case. This test can answer how good is one at avoiding false alarms.

---

### F1 (harmonic mean between precision and recall)

Formula: 

$$F1 = 2\frac{Precision*Recall}{Precision+Recall}$$


```{r, echo=FALSE, fig.width= 7, fig.height=4, dev='svg', warning=FALSE, fig.align='center'}
plot_df <- plot_df %>%
  mutate(F1 = round(2*precision*sens/(precision+sens),2))

ggplot(plot_df, aes(x = scenario, y =F1, fill = scenario)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label = F1), stat = "identity", vjust = -0.5, size = 12) +
    theme_bw() +
    scale_fill_viridis(discrete = TRUE) +
    scale_y_continuous(limits = c(0, 1.2), breaks = seq(0, 1, 0.1), labels = seq(0, 1, 0.1)) +
    labs(title = toupper("F1 for each scenario"),
         y = toupper("F1"),
         x = toupper("Scenario")) +
    theme(legend.position="none") +
    theme(text = element_text(size=20))
```

You can also apply weights to precision and recall.

---

---
class: middle

#### All the measures described above are considered to not be very good when you have imbalanced data and can be "cheated".
---
# Enter...

### Matthews correlation coefficient (MCC)

$$MCC=\frac{TP*TN-FP*FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$$
This looks like a crazy formula, but it's acctually just $\phi$, [the Person's product-moment correlation](http://www.bioinfopublication.org/files/articles/2_1_1_JMLT.pdf){*}. Values close to 1 show perfect agreement. 0 is random guessing. -1 is a model that always predicts the opposite (so it is still good, because random guessing is always the worst case scenario). 

.footnote[{*} It is common for researchers to reinvent the wheel and call a rose by multiple names]

---

```{r, echo=FALSE, fig.width= 7, fig.height=4, dev='svg', warning=FALSE, fig.align='center'}
plot_df <- plot_df %>%
  mutate_each(funs(as.numeric), fp, fn, tn, tp) %>%
  mutate(mcc = round((tp*tn-fp*fn)/sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn)),2))

ggplot(plot_df, aes(x = scenario, y =mcc, fill = scenario)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label = mcc), stat = "identity", vjust = -0.5, size = 12) +
    theme_bw() +
    scale_fill_viridis(discrete = TRUE) +
    scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.1), labels = seq(0, 1, 0.1)) +
    labs(title = toupper("MCC for each scenario"),
         y = toupper("MCC"),
         x = toupper("Scenario")) +
    theme(legend.position="none") +
    theme(text = element_text(size=20))
```

---

# Cohen's Kappa (my favorite among hard classifier metrics)

Compares the actual accuracy relative to an expected accuracy that you would observe by chance. The higher the metric, the more impresive is the performance of the classifier relative to randomly guessing the most common class. 

Was originally created to measure inter-rater agreement for qualitative variables.  

$$Kappa = \frac{A_{actual}-A_{expected}}{1-A_{expected}}$$
$$A_{expected} = {(TP*FN)*(TP*)} $$

# Thanks!

.footnote[ 
1. Slides were creaed via [**xaringan**](https://github.com/yihui/xaringan), by using [remark.js](https://remarkjs.com), [**knitr**](http://yihui.name/knitr), and [R Markdown](https://rmarkdown.rstudio.com)

2. You can access the slides on the following github [link](https://github.com/stasSajin/Slides).]

