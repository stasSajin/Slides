{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "60c4ddb9-c447-41e0-90d3-c55ae229010a"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Making a network from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "15c81c1c-17b1-4e8f-980c-6c41ff2388b8"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "* Understanding the purpose of error in a simple learning machine\n",
    "* Understanding network architecture\n",
    "* Building a three layered network:\n",
    "    - Linear Algebra\n",
    "    - How weights are used in the network\n",
    "    - How the network learns from error\n",
    "* Implementing the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e23bcdad-5969-48d6-85a3-3442f48ed6d3"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple learning machine\n",
    "Let's assume a linear relationship between number of Uber employees in an elevator and the time it takes you to get to reach the 18th Floor.\n",
    "\n",
    "![title](images/uber_example.jpg)\n",
    "\n",
    "\n",
    "$$30=5x$$\n",
    "\n",
    "How do we find x?\n",
    "\n",
    "One way of solving this problem, is to directly find x, where $x=6$ (i.e., for every uber employee in the elevator, you should expect to wait an extra 6 seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "cd7baaa8-a1f4-4848-9db6-42c33f249249"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Simple Learning Machine\n",
    "A different way of solving this problem, is to pluck in a random value and see how wrong you are?\n",
    "\n",
    "Let's use x=0.3\n",
    "\n",
    "error = 30 - 5 * 0.3\n",
    "\n",
    "error = 28.5\n",
    "\n",
    "![title](images/uber_example2.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ab4e8856-fbc6-4b22-b1e7-c8d71bbba53f"
    }
   },
   "source": [
    "### Simple Learning Machine\n",
    "Let's multiply the x by 20; 0.03*20=0.6\n",
    "\n",
    "error = 30 - 5 * 0.6\n",
    "\n",
    "error = 27\n",
    "\n",
    "![title](images/uber_example3.jpg)\n",
    "\n",
    "This is till not great. We doubled the guessed value, but our error decreased only by about 5% (1-27/28.5). If we really want to make some progress, we need to increase the value by a lot more. We can use the change in the error to figure out by how much x should increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8e4064b7-cd04-47c8-b0a4-555d7861b7dd"
    }
   },
   "source": [
    "### Things to remember:\n",
    "* We used error to modify the value of x.\n",
    "* We nudged the weight (x) based on how big our error was.\n",
    "* We don't use algebra to find the relationship between input and output. Often we will not know what that relationship is (i.e., linear, non-linear), or there might be hundreds of weights to find (making the problem algebraically intractable). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "acf3aa71-3dea-486f-9882-2e6a5e9e8882"
    }
   },
   "source": [
    "### A simple neuron ([source](http://cs231n.github.io/neural-networks-1/))\n",
    "\n",
    "![title](images/neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7d82e49e-6010-4d0d-86be-b2613d02ae25"
    }
   },
   "source": [
    "### Mathematical Formulation of the Neuron\n",
    "![title](images/neuron_model.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4d9f5677-7bae-41e4-81c6-9a6d7544b5da"
    }
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "### Sigmoid Activation Function\n",
    "![title](images/sigmoid.jpeg)\n",
    "\n",
    "$$y=\\frac{1}{1+e^{-x}}$$\n",
    "\n",
    "* Can show saturation (e.g., this happens when the output of the neuron is a value very very close to 1 or 0. The neuron will effectively stop learning when that is the case.\n",
    "* Does not zero center the output. This is an issue for deep neural architectures, since it will make gradient learning a bit more flakey. \n",
    "\n",
    "There are a lot more activation functions out there. See [this](https://stats.stackexchange.com/questions/115258/comprehensive-list-of-activation-functions-in-neural-networks-with-pros-cons) answer on CrossValidated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "1001d5f9-a2a6-4fe7-a510-f7c73bed93ce"
    }
   },
   "source": [
    "### A simple 3 layer network\n",
    "![title](images/network.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ef4a6d66-65de-40b5-a230-2c43e8720435"
    }
   },
   "source": [
    "## How signal travels through a network?\n",
    "\n",
    "![title](images/2layer_simple.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input to the first neuron in layer 2 is:\n",
    "$$x=(1*0.4) + (-0.6*0.6)$$\n",
    "$$x=0.04$$\n",
    "\n",
    "Output is:\n",
    "$$y(x)=\\frac{1}{1+e^{-0.04}}$$\n",
    "$$y(x)= 0.5099$$\n",
    "\n",
    "Input to the second neuron in layer 2 is:\n",
    "$$x=(1*0.9) + (-0.6*0.3)$$\n",
    "$$x=0.72$$\n",
    "\n",
    "Output is:\n",
    "$$y(x)=\\frac{1}{1+e^{-0.04}}$$\n",
    "$$y(x)= 0.6726$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Intro to Matrix Multiplication\n",
    "$$\\textbf{X}=\\textbf{WI}$$\n",
    "\n",
    "\n",
    " $$\n",
    "   W=\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   w_{1,1} & w_{2,1} \\\\\n",
    "   w_{1,2} & w_{2,2} \\\\\n",
    "  \\end{array} } \\right]\n",
    " $$\n",
    " \n",
    " $$\n",
    "   I=\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   i_{1} \\\\\n",
    "   i_{2} \\\\\n",
    "  \\end{array} } \\right]\n",
    " $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04,  0.72])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### matrix multiplication using numpy\n",
    "import numpy as np\n",
    "W = np.array([[0.4, 0.6], [0.9, 0.3]])\n",
    "I = np.array([1, -0.6])\n",
    "X=np.dot(W, I)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where are we?\n",
    "![title](images/network_with_output.jpg)\n",
    "\n",
    "How do we use error when multiple weights contribute to it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Weights (aka, training)\n",
    "\n",
    "![title](images/updating_weights.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Weights (aka, training)\n",
    "\n",
    "The error is being split in fractions of relative error\n",
    "\n",
    "$$Error_{w11} = e * \\frac{W_{11}}{W_{11}+W_{21}}$$\n",
    "\n",
    "$$Error_{w21} = e * \\frac{W_{21}}{W_{11}+W_{21}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagating Errors\n",
    "![title](images/error_backpropagation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Matrix Multiplication to Update Weights\n",
    " $$\n",
    "   error_{hidden}=\n",
    "  \\left[ {\\begin{array}{cc}\n",
    "   \\frac{w_{1,1}}{w_{1,1}+w_{2,1}} & \\frac{w_{1,2}}{w_{1,2}+w_{2,2}} \\\\\n",
    "   \\frac{w_{2,1}}{w_{2,1}+w_{1,1}} & \\frac{w_{2,2}}{w_{2,2}+w_{1,2}} \\\\\n",
    "  \\end{array} } \\right]\n",
    "  \\cdot\n",
    "   \\left[ {\\begin{array}{cc}\n",
    "   e_{1} \\\\\n",
    "   e_{2} \\\\\n",
    "  \\end{array} } \\right]\n",
    " $$\n",
    " \n",
    " Although mathematically normalizing the weights is the right thing to do, in practice you can acctually ignore that. This makes matrix operations much simpler, with very little cost to the time it takes to train a network.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Matrix Multiplication to Update Weights\n",
    "$$error_{hidden}= W^{T}_{hidden} * error_{output}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've gotten pretty far, so you should take a breather. \n",
    "![title](images/archer-meme-14.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What comes next is the hardest part of all, but we need to ge through it.\n",
    "![title](images/justduit.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we acctually update weights?\n",
    "Remember at the start of the talk we used the rate of change in the error rates between two iterations to adjust parameter to number of Uber employees? We'll, that's not a good way of doing it.\n",
    "\n",
    "* Normally we don't know what function we want to model (in that example we assumed a linear function with one parameter)\n",
    "* The reality is that the error likely has a complex landscape\n",
    "\n",
    "\n",
    "![title](images/error_landscape.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent High Level Explanation\n",
    "![title](images/gradient_descent_image.png)\n",
    "\n",
    "We train the network multiple times (i.e., epochs) to make sure it does not get stuck at a local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which error are we using for gradient descent?\n",
    "\n",
    "* $(target-actual)$\n",
    "* $|target-actual|$\n",
    "* $(target-actual)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which error are we using for gradient descent?\n",
    "$$(target-actual)^2$$\n",
    "\n",
    "* The error function is smooth and continuous\n",
    "* The gradient gets smaller near the minimum, so we don't overshoot the objective\n",
    "* The math is easy enough with this error\n",
    "\n",
    "You can make your own customized cost functions that include weights or that punish the model for making strong inaccurate predictions (i.e., logloss). See more [here](https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications). You can find more about their implementation by examinining the [keras source code](https://keras.io/losses/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "To do gradient descent, we need to figure out how the slope of the error relates to the change in weights. For this, we are going to use calculus.\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{jk}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "![title](images/error_backpropagation2.jpg)\n",
    "\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{jk}}=\\frac{\\partial (t_{k}-o_{k})^2}{\\partial w_{jk}}$$\n",
    "\n",
    "note that $t$ is a constant, and $o_{k}$ depends on the weights $w_{jk}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "We can rewrite\n",
    "$$\\frac{\\partial E}{\\partial w_{jk}} = \\frac{\\partial E}{\\partial o_{k}} * \\frac{\\partial o_{k}}{\\partial w_{jk}}$$\n",
    "\n",
    "Taking the derivative of the error, we get:\n",
    "$$\\frac{\\partial E}{\\partial w_{jk}} = -2(t_{k}-o_{k}) * \\frac{\\partial o_{k}}{\\partial w_{jk}}$$\n",
    "\n",
    "$o_{k}$ also need to be differentiated. Remember that o_{k} is equal with:\n",
    "$$S(\\sum_{1}^{j} w_{jk}o_{j})$$\n",
    "where, $S$ is the sigmoid.\n",
    "\n",
    "Fortunately the derivative of the sigmoid is fairly easy to compute:\n",
    "$$S(x) = S(x)(1-S(x))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "$$\\frac{\\partial E}{\\partial w_{jk}} = -2(t_{k}-o_{k}) *  S(\\sum_{1}^{j} w_{jk}o_{j}) * (1-S(\\sum_{1}^{j} w_{jk}o_{j}))* \\partial \\frac{\\sum_{1}^{j} w_{jk}o_{j}}{\\partial w_{jk}}$$\n",
    "\n",
    "We still have to take the derivative of the inner expression: $\\sum_{1}^{j} w_{jk}o_{j}$\n",
    "\n",
    "That comes to be $o_{j}$. Does anyone know why?\n",
    "\n",
    "Our final expression comes to\n",
    "$$\\frac{\\partial E}{\\partial w_{jk}} = -(t_{k}-o_{k}) *  S(\\sum_{1}^{j} w_{jk}o_{j}) * (1-S(\\sum_{1}^{j} w_{jk}o_{j}))* o_{j}$$\n",
    "\n",
    "Notice that we dropped the constant 2 because in the grand scheme of things it doesn't really channge much because, we we will see later, we use a **learning rate** parameter to adjust how fast the gradient should change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    " Let me rewquite the expression we just wrote in pseudo-english:\n",
    " \n",
    " $$\\text{partial change in error for each partial change in weights} = -(error_{output}) *  sigmoid(output) * (1-sigmoid(output)) * o_{input}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent (weight updates)\n",
    "\n",
    "When we update the weights, we don't adjust it using the full value of the gradient. We instead adjust it using a learning error paremeter.\n",
    "\n",
    "$$ w_{jk} = w_{jk} - \\alpha \\frac{\\partial E}{\\partial w_{jk}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "![title](images/gradient_descent_example_with_numbers.jpg)\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{jk}} = -(t_{k}-o_{k}) *  S(\\sum_{1}^{j} w_{jk}o_{j}) * (1-S(\\sum_{1}^{j} w_{jk}o_{j}))* o_{j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that the erros above are not really realistic. When we apply the sigmoid transformation, our values will never be larger than 1, so if you get errors larger than 1 with sigmoids, then maybe your outcome is not made out of 1/0 as it should be.\n",
    "* $-(t_{1}-o_{1})$ is equal to 1 for the first node. For the second node it is equal to 2.\n",
    "* This expression $\\sum_{j}^{1} w_{jk}o_{j}$ inside the sigmoid is equal to $3*0.8+2*0.6 = 3.6$ for the first output node and $2*0.8+2*0.6 = 2.8$ for the second output node. These are the inputs for the final layer.\n",
    "* Taking the sigmoid is simple: $\\frac{1}{1+e^{-3.6}} = 0.9734$ for the first node. For the second node, output will be $\\frac{1}{1+e^{-}} = 0.9426758$\n",
    "* the last part $o_{j}$ represents the output from the previous layer. In this case it is 0.8 ad 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "# create the neural network\n",
    "class network:\n",
    "    def __init__(self, inputnodes, hiddennodes, outputnodes, learning_rate):\n",
    "        # set the number of nodes for each input, hidden, and output layer\n",
    "        self.inodes = inputnodes\n",
    "        self.hnodes = hiddennodes\n",
    "        self.onodes = outputnodes\n",
    "        \n",
    "        # create weights linking the nodes from one layer to another\n",
    "        self.wih = np.random.normal(0.0, pow(self.hnodes, -0.5), (self.hnodes, self.inodes))\n",
    "        self.who = np.random.normal(0.0, pow(self.onodes, -0.5), (self.onodes, self.hnodes))\n",
    "        \n",
    "        # set the learning rate\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        # set the activation function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def train(self, inputs_list, target_list):\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        targets = np.array(target_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into the hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        # calculate the weights out of the hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into the final output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        # get the output errror        \n",
    "        output_errors = targets - final_outputs\n",
    "        \n",
    "        # hidden layer errors\n",
    "        hidden_errors = np.dot(self.who.T, output_errors)\n",
    "        \n",
    "        # update the weights using the backpropagation formula for the middle layer to final layer\n",
    "        self.who += self.lr * np.dot((output_errors * final_outputs * (1-final_outputs)),\n",
    "                                        np.transpose(hidden_outputs))\n",
    "        \n",
    "        # update the weights for the wih layer\n",
    "        self.wih += self.lr * np.dot((hidden_errors * hidden_outputs * (1-hidden_outputs)), np.transpose(inputs))\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def predict(self, input_list):\n",
    "        inputs = np.array(input_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into the hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        # calculate the weights out of the hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into the final output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the train and test data\n",
    "# you can get the data from here: https://www.kaggle.com/c/digit-recognizer/data\n",
    "import pandas as pd\n",
    "train = pd.read_csv('data/mnist_train.csv', header=None)\n",
    "test = pd.read_csv('data/mnist_test.csv', header=None)\n",
    "\n",
    "x_train = train.loc[:, 1:]\n",
    "y_train = train.loc[:, 0]\n",
    "x_test = test.loc[:, 1:]\n",
    "y_test = test.loc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>159</td>\n",
       "      <td>253</td>\n",
       "      <td>159</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>238</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>227</td>\n",
       "      <td>253</td>\n",
       "      <td>252</td>\n",
       "      <td>239</td>\n",
       "      <td>233</td>\n",
       "      <td>252</td>\n",
       "      <td>57</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>60</td>\n",
       "      <td>224</td>\n",
       "      <td>252</td>\n",
       "      <td>253</td>\n",
       "      <td>252</td>\n",
       "      <td>202</td>\n",
       "      <td>84</td>\n",
       "      <td>252</td>\n",
       "      <td>253</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>253</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>96</td>\n",
       "      <td>189</td>\n",
       "      <td>253</td>\n",
       "      <td>167</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>238</td>\n",
       "      <td>253</td>\n",
       "      <td>253</td>\n",
       "      <td>190</td>\n",
       "      <td>114</td>\n",
       "      <td>253</td>\n",
       "      <td>228</td>\n",
       "      <td>47</td>\n",
       "      <td>79</td>\n",
       "      <td>255</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>238</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>179</td>\n",
       "      <td>12</td>\n",
       "      <td>75</td>\n",
       "      <td>121</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>253</td>\n",
       "      <td>243</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>165</td>\n",
       "      <td>253</td>\n",
       "      <td>233</td>\n",
       "      <td>208</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>253</td>\n",
       "      <td>252</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>178</td>\n",
       "      <td>252</td>\n",
       "      <td>240</td>\n",
       "      <td>71</td>\n",
       "      <td>19</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>253</td>\n",
       "      <td>252</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>253</td>\n",
       "      <td>252</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>198</td>\n",
       "      <td>253</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>253</td>\n",
       "      <td>196</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>246</td>\n",
       "      <td>252</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>253</td>\n",
       "      <td>252</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>252</td>\n",
       "      <td>230</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>135</td>\n",
       "      <td>253</td>\n",
       "      <td>186</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>252</td>\n",
       "      <td>223</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>131</td>\n",
       "      <td>252</td>\n",
       "      <td>225</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>252</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>165</td>\n",
       "      <td>252</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>253</td>\n",
       "      <td>225</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>238</td>\n",
       "      <td>253</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>252</td>\n",
       "      <td>249</td>\n",
       "      <td>146</td>\n",
       "      <td>48</td>\n",
       "      <td>29</td>\n",
       "      <td>85</td>\n",
       "      <td>178</td>\n",
       "      <td>225</td>\n",
       "      <td>253</td>\n",
       "      <td>223</td>\n",
       "      <td>167</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>229</td>\n",
       "      <td>215</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>196</td>\n",
       "      <td>130</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>199</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>253</td>\n",
       "      <td>252</td>\n",
       "      <td>252</td>\n",
       "      <td>233</td>\n",
       "      <td>145</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>128</td>\n",
       "      <td>252</td>\n",
       "      <td>253</td>\n",
       "      <td>252</td>\n",
       "      <td>141</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1   2   3   4   5   6    7    8    9    10   11   12   13   14   15  \\\n",
       "0    0   0   0   0   0   0   0    0    0    0    0    0    0    0    0    0   \n",
       "1    0   0   0   0   0   0   0    0    0    0    0    0    0    0    0    0   \n",
       "2    0   0   0   0   0   0   0    0    0    0    0    0    0    0    0    0   \n",
       "3    0   0   0   0   0   0   0    0    0    0    0    0    0    0    0    0   \n",
       "4    0   0   0   0   0   0   0    0    0    0    0    0    0    0    0   51   \n",
       "5    0   0   0   0   0   0   0    0    0    0    0    0    0    0   48  238   \n",
       "6    0   0   0   0   0   0   0    0    0    0    0    0    0   54  227  253   \n",
       "7    0   0   0   0   0   0   0    0    0    0    0   10   60  224  252  253   \n",
       "8    0   0   0   0   0   0   0    0    0    0    0  163  252  252  252  253   \n",
       "9    0   0   0   0   0   0   0    0    0    0   51  238  253  253  190  114   \n",
       "10   0   0   0   0   0   0   0    0    0   48  238  252  252  179   12   75   \n",
       "11   0   0   0   0   0   0   0    0   38  165  253  233  208   84    0    0   \n",
       "12   0   0   0   0   0   0   0    7  178  252  240   71   19   28    0    0   \n",
       "13   0   0   0   0   0   0   0   57  252  252   63    0    0    0    0    0   \n",
       "14   0   0   0   0   0   0   0  198  253  190    0    0    0    0    0    0   \n",
       "15   0   0   0   0   0   0  76  246  252  112    0    0    0    0    0    0   \n",
       "16   0   0   0   0   0   0  85  252  230   25    0    0    0    0    0    0   \n",
       "17   0   0   0   0   0   0  85  252  223    0    0    0    0    0    0    0   \n",
       "18   0   0   0   0   0   0  85  252  145    0    0    0    0    0    0    0   \n",
       "19   0   0   0   0   0   0  86  253  225    0    0    0    0    0    0  114   \n",
       "20   0   0   0   0   0   0  85  252  249  146   48   29   85  178  225  253   \n",
       "21   0   0   0   0   0   0  85  252  252  252  229  215  252  252  252  196   \n",
       "22   0   0   0   0   0   0  28  199  252  252  253  252  252  233  145    0   \n",
       "23   0   0   0   0   0   0   0   25  128  252  253  252  141   37    0    0   \n",
       "24   0   0   0   0   0   0   0    0    0    0    0    0    0    0    0    0   \n",
       "25   0   0   0   0   0   0   0    0    0    0    0    0    0    0    0    0   \n",
       "26   0   0   0   0   0   0   0    0    0    0    0    0    0    0    0    0   \n",
       "27   0   0   0   0   0   0   0    0    0    0    0    0    0    0    0    0   \n",
       "\n",
       "     16   17   18   19   20   21   22  23  24  25  26  27  \n",
       "0     0    0    0    0    0    0    0   0   0   0   0   0  \n",
       "1     0    0    0    0    0    0    0   0   0   0   0   0  \n",
       "2     0    0    0    0    0    0    0   0   0   0   0   0  \n",
       "3     0    0    0    0    0    0    0   0   0   0   0   0  \n",
       "4   159  253  159   50    0    0    0   0   0   0   0   0  \n",
       "5   252  252  252  237    0    0    0   0   0   0   0   0  \n",
       "6   252  239  233  252   57    6    0   0   0   0   0   0  \n",
       "7   252  202   84  252  253  122    0   0   0   0   0   0  \n",
       "8   252  252   96  189  253  167    0   0   0   0   0   0  \n",
       "9   253  228   47   79  255  168    0   0   0   0   0   0  \n",
       "10  121   21    0    0  253  243   50   0   0   0   0   0  \n",
       "11    0    0    0    0  253  252  165   0   0   0   0   0  \n",
       "12    0    0    0    0  253  252  195   0   0   0   0   0  \n",
       "13    0    0    0    0  253  252  195   0   0   0   0   0  \n",
       "14    0    0    0    0  255  253  196   0   0   0   0   0  \n",
       "15    0    0    0    0  253  252  148   0   0   0   0   0  \n",
       "16    0    0    7  135  253  186   12   0   0   0   0   0  \n",
       "17    0    7  131  252  225   71    0   0   0   0   0   0  \n",
       "18   48  165  252  173    0    0    0   0   0   0   0   0  \n",
       "19  238  253  162    0    0    0    0   0   0   0   0   0  \n",
       "20  223  167   56    0    0    0    0   0   0   0   0   0  \n",
       "21  130    0    0    0    0    0    0   0   0   0   0   0  \n",
       "22    0    0    0    0    0    0    0   0   0   0   0   0  \n",
       "23    0    0    0    0    0    0    0   0   0   0   0   0  \n",
       "24    0    0    0    0    0    0    0   0   0   0   0   0  \n",
       "25    0    0    0    0    0    0    0   0   0   0   0   0  \n",
       "26    0    0    0    0    0    0    0   0   0   0   0   0  \n",
       "27    0    0    0    0    0    0    0   0   0   0   0   0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', 30)\n",
    "pd.DataFrame(np.array(x_train.iloc[1]).reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual outcome value: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10b08bb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot an example\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index = 1\n",
    "image_array = np.asfarray(x_train.iloc[index].values.reshape((28, 28)))\n",
    "plt.imshow(image_array, cmap='Greys', interpolation = 'None')\n",
    "print(\"actual outcome value: {}\".format(y_train[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the input\n",
    "x_train_scaled = x_train/255 * 0.99 + 0.01\n",
    "x_test_scaled = x_test/255 * 0.99 + 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting up the network\n",
    "input_nodes = 784\n",
    "hidden_nodes = 100\n",
    "output_nodes = 10\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 0.2\n",
    "\n",
    "# create the instance of the model\n",
    "net = network(input_nodes, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "\n",
    "train_targets = np.zeros((len(y_train), output_nodes)) + 0.01\n",
    "train_targets[np.array(range(0, len(y_train))), y_train.values] = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are implementing a type of gradient descent called stochastic gradient descent. This basically means that we are updating the weights after each training example. There are other types of gradient descent, and I highly recommend reading this paper by [Ruder (2016)](https://arxiv.org/pdf/1609.04747.pdf) for an overview on different types of gradient descent.\n",
    "\n",
    "Stochastic gradient descent has a few benefits.\n",
    "* It is usually faster. You converge to an optimal solution in a few epochs. \n",
    "* You can implement the algorithm in online learning examples.\n",
    "\n",
    "Obviously, it is not the best numerical optimization algorithm out there. I recommend reading about ADAM [(Kingma, Lei Ba 2015)](https://arxiv.org/pdf/1412.6980.pdf), which implements momentum and \"memory\" like strategies that allow for very rapid convergence with smaller likelihood at getting stuck within sub-optimal solutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, outcome in zip(x_train_scaled.values, train_targets):\n",
    "    net.train(inputs_list=features, target_list=outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observed accuracy 94.93\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(net.predict(x_test_scaled).T, axis=1)\n",
    "accuracy = np.mean(predictions == y_test.values)\n",
    "print(\"observed accuracy {}\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Starting epoch 2\n",
      "Starting epoch 3\n",
      "Starting epoch 4\n"
     ]
    }
   ],
   "source": [
    "### train for more epochs\n",
    "epochs = 4\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Starting epoch {}\".format(epoch+1))\n",
    "    for features, outcome in zip(x_train_scaled.values, train_targets):\n",
    "        net.train(inputs_list=features, target_list=outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observed accuracy 95.87\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(net.predict(x_test_scaled).T, axis=1)\n",
    "accuracy = np.mean(predictions == y_test.values)\n",
    "print(\"observed accuracy {}\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/2\n",
      "60000/60000 [==============================] - 85s - loss: 0.0656 - acc: 0.6225 - val_loss: 0.0446 - val_acc: 0.8035\n",
      "Epoch 2/2\n",
      "60000/60000 [==============================] - 85s - loss: 0.0374 - acc: 0.8349 - val_loss: 0.0309 - val_acc: 0.8694\n",
      "Test loss: 0.0309349358231\n",
      "Test accuracy: 0.8694\n"
     ]
    }
   ],
   "source": [
    "# Doing the same thing with an open source library: [Keras](https://keras.io/)\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "num_classes = 10\n",
    "epochs = 2\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation='sigmoid', input_shape=(784,)))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer=SGD(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [capstone]",
   "language": "python",
   "name": "Python [capstone]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "nbpresent": {
   "slides": {
    "253d1848-1959-4281-991e-9da95c5dd9c5": {
     "id": "253d1848-1959-4281-991e-9da95c5dd9c5",
     "prev": "3bfdbce5-571d-4234-a0d9-58f6400c9f26",
     "regions": {
      "180433f7-0fc6-4260-953b-6258e063cefd": {
       "attrs": {
        "height": 0.8,
        "width": 0.45,
        "x": 0.5,
        "y": 0.1
       },
       "content": {
        "cell": "7d82e49e-6010-4d0d-86be-b2613d02ae25",
        "part": "whole"
       },
       "id": "180433f7-0fc6-4260-953b-6258e063cefd"
      },
      "f2d6e8f7-ae9b-4627-8f62-42419b70dc95": {
       "attrs": {
        "height": 0.8,
        "width": 0.45,
        "x": 0.05,
        "y": 0.1
       },
       "content": {
        "cell": "acf3aa71-3dea-486f-9882-2e6a5e9e8882",
        "part": "whole"
       },
       "id": "f2d6e8f7-ae9b-4627-8f62-42419b70dc95"
      }
     }
    },
    "3bfdbce5-571d-4234-a0d9-58f6400c9f26": {
     "id": "3bfdbce5-571d-4234-a0d9-58f6400c9f26",
     "prev": "d27cedc5-2003-4145-9550-5e1ef7320892",
     "regions": {
      "efc43c37-c168-4610-b6dd-bc8f9fc31fc0": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "8e4064b7-cd04-47c8-b0a4-555d7861b7dd",
        "part": "whole"
       },
       "id": "efc43c37-c168-4610-b6dd-bc8f9fc31fc0"
      }
     }
    },
    "4ec71f27-e331-4de5-b3ec-4610c3f1fa42": {
     "id": "4ec71f27-e331-4de5-b3ec-4610c3f1fa42",
     "prev": "253d1848-1959-4281-991e-9da95c5dd9c5",
     "regions": {
      "212478cc-6736-4042-9b8b-408757752a6b": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "4d9f5677-7bae-41e4-81c6-9a6d7544b5da",
        "part": "whole"
       },
       "id": "212478cc-6736-4042-9b8b-408757752a6b"
      }
     }
    },
    "5ce10036-f196-4f25-95af-49bda22e4925": {
     "id": "5ce10036-f196-4f25-95af-49bda22e4925",
     "prev": "ba8c86a6-cad5-41a7-bb80-1d3503b04209",
     "regions": {
      "999d5267-b5cf-4059-9a3f-6cb8b51191b0": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "cd7baaa8-a1f4-4848-9db6-42c33f249249",
        "part": "whole"
       },
       "id": "999d5267-b5cf-4059-9a3f-6cb8b51191b0"
      }
     }
    },
    "73be2ff5-b5ef-4b03-a713-115a6eb20d80": {
     "id": "73be2ff5-b5ef-4b03-a713-115a6eb20d80",
     "prev": "c92f1ba8-aa61-4903-9849-a1fbeb7a75d1",
     "regions": {
      "0dc30df1-30d6-4dae-b508-770f7e825c7a": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "15c81c1c-17b1-4e8f-980c-6c41ff2388b8",
        "part": "whole"
       },
       "id": "0dc30df1-30d6-4dae-b508-770f7e825c7a"
      }
     }
    },
    "ba8c86a6-cad5-41a7-bb80-1d3503b04209": {
     "id": "ba8c86a6-cad5-41a7-bb80-1d3503b04209",
     "layout": "manual",
     "prev": "73be2ff5-b5ef-4b03-a713-115a6eb20d80",
     "regions": {
      "f32630e7-cb42-4c50-b0ed-67a01c334639": {
       "attrs": {
        "height": 0.8978593930909271,
        "width": 0.9397050780376135,
        "x": 0.021193020523760254,
        "y": 0.03049764277029454
       },
       "content": {
        "cell": "e23bcdad-5969-48d6-85a3-3442f48ed6d3",
        "part": "whole"
       },
       "id": "f32630e7-cb42-4c50-b0ed-67a01c334639"
      }
     },
     "theme": "663e73a7-b6c7-44cf-9a9d-7b350e54fed2"
    },
    "c92f1ba8-aa61-4903-9849-a1fbeb7a75d1": {
     "id": "c92f1ba8-aa61-4903-9849-a1fbeb7a75d1",
     "prev": null,
     "regions": {
      "8c999856-da6f-45fd-a36c-c8dfc2293548": {
       "attrs": {
        "height": 0.10718033043614447,
        "width": 0.7316420014094434,
        "x": 0.04432699083861881,
        "y": 0.16890611541774336
       },
       "content": {
        "cell": "60c4ddb9-c447-41e0-90d3-c55ae229010a",
        "part": "whole"
       },
       "id": "8c999856-da6f-45fd-a36c-c8dfc2293548"
      }
     },
     "theme": null
    },
    "d27cedc5-2003-4145-9550-5e1ef7320892": {
     "id": "d27cedc5-2003-4145-9550-5e1ef7320892",
     "prev": "5ce10036-f196-4f25-95af-49bda22e4925",
     "regions": {
      "d0787dc2-b5d4-4808-8e2b-714ca270d6c7": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": 0,
        "y": 0
       },
       "content": {
        "cell": "ab4e8856-fbc6-4b22-b1e7-c8d71bbba53f",
        "part": "whole"
       },
       "id": "d0787dc2-b5d4-4808-8e2b-714ca270d6c7"
      }
     }
    }
   },
   "themes": {
    "default": "67888816-e2c1-41e7-aa5a-f66250880105",
    "theme": {
     "67888816-e2c1-41e7-aa5a-f66250880105": {
      "id": "67888816-e2c1-41e7-aa5a-f66250880105",
      "palette": {
       "19cc588f-0593-49c9-9f4b-e4d7cc113b1c": {
        "id": "19cc588f-0593-49c9-9f4b-e4d7cc113b1c",
        "rgb": [
         252,
         252,
         252
        ]
       },
       "31af15d2-7e15-44c5-ab5e-e04b16a89eff": {
        "id": "31af15d2-7e15-44c5-ab5e-e04b16a89eff",
        "rgb": [
         68,
         68,
         68
        ]
       },
       "50f92c45-a630-455b-aec3-788680ec7410": {
        "id": "50f92c45-a630-455b-aec3-788680ec7410",
        "rgb": [
         155,
         177,
         192
        ]
       },
       "c5cc3653-2ee1-402a-aba2-7caae1da4f6c": {
        "id": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "rgb": [
         43,
         126,
         184
        ]
       },
       "efa7f048-9acb-414c-8b04-a26811511a21": {
        "id": "efa7f048-9acb-414c-8b04-a26811511a21",
        "rgb": [
         25.118061674008803,
         73.60176211453744,
         107.4819383259912
        ]
       }
      },
      "rules": {
       "blockquote": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410"
       },
       "code": {
        "font-family": "Anonymous Pro"
       },
       "h1": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 8
       },
       "h2": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 6
       },
       "h3": {
        "color": "50f92c45-a630-455b-aec3-788680ec7410",
        "font-family": "Lato",
        "font-size": 5.5
       },
       "h4": {
        "color": "c5cc3653-2ee1-402a-aba2-7caae1da4f6c",
        "font-family": "Lato",
        "font-size": 5
       },
       "h5": {
        "font-family": "Lato"
       },
       "h6": {
        "font-family": "Lato"
       },
       "h7": {
        "font-family": "Lato"
       },
       "pre": {
        "font-family": "Anonymous Pro",
        "font-size": 4
       }
      },
      "text-base": {
       "font-family": "Merriweather",
       "font-size": 4
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
